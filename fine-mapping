#!/usr/bin/env python3
"""
exwas_finemap.py
================

Fine-mapping pipeline for UKB OQFE WES ExWAS (vigorous vs sedentary).

Given association summary stats (e.g., PLINK2 --glm combined across chromosomes)
and a WES PLINK reference panel (same sample set you used for ExWAS), this script:

1) Defines loci by clumping lead variants (PLINK2) or by a P-value threshold.
2) For each lead variant, builds a +/- window (e.g., 500 kb), extracts variants, and
   computes an LD matrix from WES PLINK genotypes.
3) Prepares FINEMAP input files (z file, LD matrix, master file) and can run FINEMAP.
4) Optionally writes/executes a small R script that runs SuSiE fine-mapping using the
   same per-locus z and LD files.

Typical usage (PLINK2 summaries):
---------------------------------
python exwas_finemap.py \
  --sumstats results/plink2/vigorous_activity_norm_inv.plink2_glm.tsv \
  --phenotype vigorous_activity_norm_inv \
  --wes-prefix /data/ukb23158_c{CHR}_b0_v1 \
  --keep results/plink2/samples.keep \
  --outdir finemap_vig_vs_sed \
  --n 120000 \
  --p-thresh 5e-8 \
  --finemap-exe /path/to/finemap_v1.4_x86_64

If you prefer REGENIE output, pass its combined table to --sumstats and map columns
with --beta-col/--se-col/--p-col/--id-col as needed.

Notes:
- Requires PLINK2 in PATH for clumping and LD.
- For FINEMAP run: provide --finemap-exe.
- For SuSiE: requires Rscript with susieR installed; add --run-susie.
"""

import os, sys, argparse, subprocess, tempfile, shutil, gzip, json
import pandas as pd
import numpy as np

# --------------------------- helpers ---------------------------

def must_exist(path):
    if path is None or not os.path.exists(path):
        raise FileNotFoundError(f"Missing file: {path}")
    return path

def run(cmd, cwd=None):
    print("[cmd]", " ".join(cmd))
    subprocess.check_call(cmd, cwd=cwd)

def read_sumstats(path, cols):
    # cols: dict mapping: required keys = id, chr, pos, a1, a2, beta, se, p, n (n optional if provided via --n)
    df = pd.read_csv(path, sep="\t")
    missing = [k for k,v in cols.items() if v not in df.columns]
    if missing:
        raise ValueError(f"Missing required columns in sumstats: {missing}")
    out = df.rename(columns={cols["id"]:"SNP",
                             cols["chr"]:"CHR",
                             cols["pos"]:"BP",
                             cols["a1"]:"A1",
                             cols["a2"]:"A2",
                             cols["beta"]:"BETA",
                             cols["se"]:"SE",
                             cols["p"]:"P"})
    if "n" in cols and cols["n"] in df.columns:
        out["N"] = df[cols["n"]]
    elif "OBS_CT" in df.columns:
        out["N"] = df["OBS_CT"]
    else:
        out["N"] = np.nan
    return out

def plink2_clump(sumstats_tsv, wes_prefix, keep, outprefix, p1=5e-8, p2=1e-6, r2=0.1, kb=500):
    # Use one chromosome (any) to get .bim header? Not requiredâ€”plink2 clump reads --bfile directly.
    # We need a .assoc-like file; PLINK2 can clump with --clump on a generic file if we tell it column names.
    # Create a clump input with required headers: CHR BP SNP P
    tmp = outprefix + ".clumpinput.tsv"
    df = pd.read_csv(sumstats_tsv, sep="\t")
    need = ["CHR","BP","SNP","P"]
    if not all(c in df.columns for c in need):
        raise ValueError("sumstats_tsv must have columns CHR,BP,SNP,P for clumping.")
    df[need].to_csv(tmp, sep="\t", index=False)
    # Build a merged bed prefix for PLINK2? We assume per-chr prefixes: wes_prefix with {CHR}.
    # PLINK2 --clump needs a single dataset; we can run clump per chr and concatenate.
    clumped = []
    for chrn in list(map(str, range(1,23))):
        bfile = wes_prefix.replace("{CHR}", chrn)
        if not (os.path.exists(bfile+".bed") or os.path.exists(bfile+".pgen")):
            continue
        out = f"{outprefix}.chr{chrn}"
        cmd = [
            "plink2",
            "--bfile", bfile,
            "--keep", keep,
            "--clump", tmp,
            "--clump-snp-field", "SNP",
            "--clump-field", "P",
            "--clump-p1", str(p1),
            "--clump-p2", str(p2),
            "--clump-r2", str(r2),
            "--clump-kb", str(kb),
            "--out", out
        ]
        run(cmd)
        clumped.append(out+".clumps")
    # Combine
    leads = []
    for path in clumped:
        if os.path.exists(path):
            t = pd.read_csv(path, delim_whitespace=True, comment="#")
            # PLINK2 .clumps may have varying spaces; safer to read via engine python? but ok.
            if "SNP" in t.columns and "CHR" in t.columns and "BP" in t.columns:
                leads.append(t[["CHR","BP","SNP","P"]])
    if not leads:
        raise RuntimeError("No clumped lead variants found.")
    all_leads = pd.concat(leads, ignore_index=True).drop_duplicates("SNP")
    lead_path = outprefix + ".lead.tsv"
    all_leads.to_csv(lead_path, sep="\t", index=False)
    return lead_path, all_leads

def extract_region_and_ld(chrn, start, end, wes_prefix, keep, outprefix):
    bfile = wes_prefix.replace("{CHR}", str(chrn))
    region = f"{chrn}:{start}-{end}"
    # Extract region variants and compute LD matrix (correlation) among them
    # Output: .pgen set + LD square
    cmd = [
        "plink2",
        "--bfile", bfile,
        "--keep", keep,
        "--chr", str(chrn),
        "--from-bp", str(start),
        "--to-bp", str(end),
        "--make-bed",
        "--out", outprefix
    ]
    run(cmd)
    # Compute LD (correlation, not r^2)
    cmd_ld = [
        "plink2",
        "--bfile", outprefix,
        "--r", "square", "gz",
        "--out", outprefix
    ]
    run(cmd_ld)
    return outprefix+".bed", outprefix+".bim", outprefix+".fam", outprefix+".ld.gz"

def make_finemap_inputs(locus_df, ld_gz, n, outprefix):
    """
    FINEMAP z format needs:
    rsid chromosome position allele1 allele2 maf beta se n
    """
    # Compute MAF from allele frequency if available; otherwise estimate from SE/BETA? Better: get freq via PLINK.
    # Here we allow MAF missing -> set to 0.01 as placeholder is bad. Instead, try to read freq via plink2.
    # locus_df should include SNP, CHR, BP, A1, A2, BETA, SE, P, (optional MAF).
    use = locus_df.copy()
    cols_needed = ["SNP","CHR","BP","A1","A2","BETA","SE"]
    if not all(c in use.columns for c in cols_needed):
        raise ValueError("locus_df missing required columns for FINEMAP z.")
    if "MAF" not in use.columns:
        # If no MAF provided, create dummy; users can replace with frequencies. We'll warn.
        use["MAF"] = np.nan
        print("[warn] MAF missing in sumstats; consider adding allele frequencies for FINEMAP calibration.")
    use["N"] = n
    z = use[["SNP","CHR","BP","A1","A2","MAF","BETA","SE","N"]].copy()
    z.columns = ["rsid","chromosome","position","allele1","allele2","maf","beta","se","n"]
    z_path = outprefix + ".z"
    z.to_csv(z_path, sep=" ", index=False)
    # LD file (text) from gz square
    ld_txt = outprefix + ".ld"
    with gzip.open(ld_gz, "rt") as fin, open(ld_txt, "w") as fout:
        shutil.copyfileobj(fin, fout)
    # SNP list file in the same order as LD (plink prints variant IDs order). We'll reconstruct order via .bim
    # FINEMAP requires .snp file listing rsids (one per line) matching the LD order
    # We expect the LD matrix order equals the .bim variant order used in --r square
    bim = pd.read_csv(outprefix+".bim", sep="\t", header=None,
                      names=["CHR","SNP","CM","BP","A1","A2"])
    snp_path = outprefix + ".snp"
    bim["SNP"].to_csv(snp_path, index=False, header=False)
    # Master file
    master = outprefix + ".master"
    with open(master, "w") as fh:
        fh.write("z;ld;snp;config;cred;log;n_samples\n")
        fh.write(f"{os.path.basename(z_path)};{os.path.basename(ld_txt)};{os.path.basename(snp_path)};{os.path.basename(outprefix)}.config;{os.path.basename(outprefix)}.cred;{os.path.basename(outprefix)}.log;{int(n)}\n")
    return {"z": z_path, "ld": ld_txt, "snp": snp_path, "master": master}

def write_susie_runner(outprefix, l=10, coverage=0.95):
    r_path = outprefix + ".susie.R"
    code = f"""
args <- commandArgs(trailingOnly=TRUE)
z_file <- args[1]
ld_file <- args[2]
out_prefix <- args[3]
L <- as.integer(args[4])
coverage <- as.numeric(args[5])

suppressPackageStartupMessages({{
  library(data.table)
  library(Matrix)
  library(susieR)
}})

z <- fread(z_file, header=TRUE)
# Read LD as dense matrix
LD <- as.matrix(fread(ld_file, header=FALSE))
# Convert to z-scores: z = beta/se (already implied by inputs)
zscore <- z$beta / z$se
# Run SuSiE with supplied LD and z-scores
fit <- susie_rss(zscore, R=LD, L=L, coverage=coverage, refine=TRUE)

# Credible sets
cs <- susie_get_cs(fit, Xcorr=LD)
saveRDS(fit, file=paste0(out_prefix, ".susie.fit.rds"))
saveRDS(cs,  file=paste0(out_prefix, ".susie.cs.rds"))

# Write a simple table of variants with PIP
pip <- data.frame(rsid=z$rsid, pip=fit$pip)
data.table::fwrite(pip, file=paste0(out_prefix, ".susie.pip.tsv"), sep="\\t")
"""
    with open(r_path, "w") as fh:
        fh.write(code)
    return r_path

# --------------------------- main ---------------------------

def main():
    ap = argparse.ArgumentParser(description="Fine-map ExWAS loci (vigorous vs sedentary) using WES LD.")
    ap.add_argument("--sumstats", required=True, help="Combined association table (PLINK2 .glm.linear combined or REGENIE step2 combined)")
    ap.add_argument("--phenotype", required=False, help="Phenotype name (for filtering if your file has mixed traits)")
    ap.add_argument("--wes-prefix", required=True, help="WES PLINK prefix with {CHR}")
    ap.add_argument("--keep", required=True, help="FID IID keep file matching your .phe samples")
    ap.add_argument("--outdir", required=True, help="Output directory for fine-mapping")
    ap.add_argument("--n", type=int, required=True, help="Sample size (effective N) for this analysis")
    # Locus definition
    ap.add_argument("--p-thresh", type=float, default=5e-8, help="Lead variant p-value threshold for locus seeds")
    ap.add_argument("--window-kb", type=int, default=500, help="+/- window size around lead (kb)")
    ap.add_argument("--clump-r2", type=float, default=0.1, help="Clumping r^2")
    ap.add_argument("--clump-kb", type=int, default=500, help="Clumping kb")
    ap.add_argument("--clump-p2", type=float, default=1e-6, help="Secondary p for clumping")
    # Column mapping (defaults for PLINK2 --glm output)
    ap.add_argument("--id-col", default="ID", help="Variant ID column (PLINK2: ID)")
    ap.add_argument("--chr-col", default="#CHROM", help="Chromosome column (PLINK2: #CHROM)")
    ap.add_argument("--pos-col", default="POS", help="Position column (PLINK2: POS)")
    ap.add_argument("--a1-col", default="A1", help="Effect allele column")
    ap.add_argument("--a2-col", default="A2", help="Other allele column")
    ap.add_argument("--beta-col", default="BETA", help="Effect size column")
    ap.add_argument("--se-col", default="SE", help="Standard error column")
    ap.add_argument("--p-col", default="P", help="P-value column")
    ap.add_argument("--n-col", default=None, help="Sample size column (else uses --n)")
    # FINEMAP / SuSiE
    ap.add_argument("--finemap-exe", default=None, help="Path to FINEMAP executable to run it")
    ap.add_argument("--run-susie", action="store_true", help="Also run a SuSiE pass via R (requires susieR)")

    args = ap.parse_args()
    os.makedirs(args.outdir, exist_ok=True)

    # 1) Load sumstats and standardize columns
    cols = {"id": args.id_col, "chr": args.chr_col, "pos": args.pos_col,
            "a1": args.a1_col, "a2": args.a2_col, "beta": args.beta_col,
            "se": args.se_col, "p": args.p_col}
    if args.n_col:
        cols["n"] = args.n_col
    df = read_sumstats(args.sumstats, cols)
    # Keep only rows with non-missing BETA/SE
    df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=["BETA","SE","P"])
    # PLINK2 sometimes labels chr as strings; coerce
    df["CHR"] = df["CHR"].astype(str).str.replace("chr","", regex=False)
    df["CHR"] = df["CHR"].replace({"X":"23","Y":"24"})
    df["CHR"] = pd.to_numeric(df["CHR"], errors="coerce")
    df = df.dropna(subset=["CHR","BP"]).astype({"CHR":int, "BP":int})

    # If phenotype filter is needed (e.g., multiple traits in one table)
    if args.phenotype and "PHENO" in df.columns:
        df = df[df["PHENO"]==args.phenotype].copy()

    # 2) Identify lead variants by clumping
    clump_prefix = os.path.join(args.outdir, "clump")
    # Write a minimal sumstats file for clumping with required columns
    clump_in = os.path.join(args.outdir, "clump_input.tsv")
    df[["CHR","BP","SNP","P"]].to_csv(clump_in, sep="\t", index=False)
    lead_path, leads = plink2_clump(clump_in, args.wes_prefix, args.keep, clump_prefix,
                                    p1=args.p_thresh, p2=args.clump_p2, r2=args.clump_r2, kb=args.clump_kb)
    print(f"[ok] Lead variants: {lead_path} ({len(leads)} loci)")

    # 3) For each lead, build locus, compute LD, and prepare FINEMAP inputs
    results = []
    for _, row in leads.iterrows():
        chrn = int(row["CHR"])
        bp = int(row["BP"])
        start = max(1, bp - args.window_kb*1000)
        end   = bp + args.window_kb*1000
        locus_tag = f"chr{chrn}_{bp}_{args.window_kb}kb"
        locus_dir = os.path.join(args.outdir, locus_tag)
        os.makedirs(locus_dir, exist_ok=True)
        # Extract region LD
        bed, bim, fam, ld_gz = extract_region_and_ld(chrn, start, end, args.wes_prefix, args.keep, os.path.join(locus_dir, "locus"))
        # Subset sumstats to region & to variants present in .bim
        bim_df = pd.read_csv(bim, sep="\t", header=None, names=["CHR","SNP","CM","BP","A1b","A2b"])
        set_snps = set(bim_df["SNP"])
        sub = df[(df["CHR"]==chrn) & (df["BP"]>=start) & (df["BP"]<=end) & (df["SNP"].isin(set_snps))].copy()
        if sub.empty:
            print(f"[warn] No overlapping sumstats in {locus_tag}; skipping.")
            continue
        # Prepare FINEMAP z/ld/snp/master
        fin = make_finemap_inputs(sub, ld_gz, args.n, os.path.join(locus_dir, "finemap"))
        # Optionally run FINEMAP
        if args.finemap_exe:
            run([args.finemap_exe, "--sss", "--in-files", os.path.basename(fin["master"])], cwd=locus_dir)
        # Optionally run SuSiE
        if args.run_susie:
            rfile = write_susie_runner(os.path.join(locus_dir, "finemap"))
            run(["Rscript", os.path.basename(rfile), os.path.basename(fin["z"]), os.path.basename(fin["ld"]), "finemap", "10", "0.95"], cwd=locus_dir)
        results.append({"locus": locus_tag, "n_snps": sub.shape[0], "start": start, "end": end})

    # 4) Manifest
    man = pd.DataFrame(results)
    man.to_csv(os.path.join(args.outdir, "loci_manifest.tsv"), sep="\t", index=False)
    print(f"[done] Fine-mapping setup in {args.outdir}. Loci: {len(results)}")

if __name__ == "__main__":
    main()
